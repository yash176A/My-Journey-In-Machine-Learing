 üéØ What is PCA (Principal Component Analysis)?

**Principal Component Analysis (PCA)** is an **unsupervised machine learning technique** used for **dimensionality reduction**. It transforms the original data into a new set of axes (called **principal components**) that capture the **most important features** (variance) of the data.

 ‚ùì Why is PCA used?
1. **Dimensionality Reduction**:

   * Many datasets have a **large number of variables** (features), some of which may be redundant or irrelevant.
   * PCA helps to **reduce the number of features** while keeping as much **useful information (variance)** as possible.

2. **Data Visualization**:

   * High-dimensional data is hard to visualize.
   * PCA reduces data to **2D or 3D** for easy visualization.

3. **Noise Reduction**:

   * PCA filters out noise by focusing only on **principal components** that carry **significant variance**.

4. **Faster Computations**:

   * Reducing the number of features helps **speed up training** of machine learning models.

5. **Avoid Multicollinearity**:

   * In datasets where features are **highly correlated**, PCA transforms them into a set of **uncorrelated components**.

üìç Where is PCA used?

| Field                       | Application Example                           |
| --------------------------- | --------------------------------------------- |
| **Machine Learning**        | Preprocessing before model training           |
| **Computer Vision**         | Image compression and feature extraction      |
| **Bioinformatics**          | Gene expression data analysis                 |
| **Finance**                 | Portfolio optimization, risk modeling         |
| **Marketing**               | Customer segmentation                         |
| **Psychology / Social Sci** | Reducing survey variables for factor analysis |
| **Robotics**                | Sensor data compression and analysis          |
| **Signal Processing**       | Dimensionality reduction of time-series data  |

üìä What kind of data is used for PCA?

* PCA works best with **continuous numerical data**.
* It assumes the data is **linearly correlated**.
* Input data should be **scaled or normalized** before applying PCA, especially if features are on different scales.

‚úî Suitable data:

* Sensor readings
* Image pixel intensities
* Financial time series
* Tabular datasets with many correlated numerical features

 ‚ùå Not suitable for:

* Categorical data (unless encoded)
* Data with non-linear relationships (unless kernel PCA is used)

‚öôÔ∏è How does PCA work?

 1. **Standardize the data**:

* Make all features have **mean = 0** and **standard deviation = 1**.

 2. **Compute the Covariance Matrix**:

* Measures how variables vary together.

 3. **Calculate Eigenvectors and Eigenvalues**:

* Eigenvectors represent the **principal components** (directions).
* Eigenvalues show how much **variance** is captured by each component.

 4. **Select Top k Components**:

* Choose the **k** principal components that capture the **most variance**.

 5. **Project Data**:

* Transform original data into the new k-dimensional space using these components.

üß† Intuition of PCA:

Imagine you have a cloud of points in 3D space. PCA finds the **direction where the points are spread out the most** ‚Äî that becomes PC1. The second direction (PC2) is perpendicular to the first and captures the next most variance. This continues for all dimensions.

---

 üìê Mathematical Representation:

Given a data matrix **X** (n samples √ó p features):

1. Center the data:

   $$
   X_{centered} = X - \bar{X}
   $$

2. Compute covariance matrix:

   $$
   \Sigma = \frac{1}{n - 1} X_{centered}^T X_{centered}
   $$

3. Find eigenvalues (Œª) and eigenvectors (v):

   $$
   \Sigma v = \lambda v
   $$

4. Sort eigenvectors by decreasing eigenvalues (variance captured).

5. Select top **k** eigenvectors to form **projection matrix W**.

6. Transform original data:

   $$
   Z = X_{centered} \cdot W
   $$

---

üìå Pros and Cons of PCA

‚úÖ Pros:

* Reduces dimensionality
* Removes multicollinearity
* Speeds up computation
* Helps visualization
* Highlights most informative features

‚ùå Cons:

* Loses interpretability of features
* Only captures **linear** relationships
* Sensitive to scaling
* Not ideal for categorical data

---

üîÅ Variants of PCA:

| Variant             | Description                                                                |
| ------------------- | -------------------------------------------------------------------------- |
| **Kernel PCA**      | Captures **non-linear** structures in the data using kernel functions.     |
| **Sparse PCA**      | Produces components with fewer non-zero loadings for **interpretability**. |
| **Incremental PCA** | Used for **large datasets** that don‚Äôt fit in memory.                      |

---

üìé PCA in Python (Sklearn Example):

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Load data
data = pd.read_csv('your_data.csv')

# Standardize
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

# Apply PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(data_scaled)

# Explained variance
print(pca.explained_variance_ratio_)
```

---

üßæ Conclusion:

**PCA is a powerful technique** for understanding and reducing high-dimensional data by focusing on the most informative components. It's a fundamental tool in data science, used in numerous fields for better analysis, modeling, and visualization.

** CODE **

